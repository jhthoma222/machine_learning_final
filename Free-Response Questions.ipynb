{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron Submission Free-Response Questions\n",
    "\n",
    "\n",
    "A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!\n",
    "\n",
    "\n",
    "When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.\n",
    "\n",
    "\n",
    "Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers.  \n",
    "\n",
    "\n",
    "We can’t wait to see what you’ve put together for this project!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "\n",
    "#### Answer 1\n",
    "  The goal of this project is to use machine learning to be a detective to try and identify persons of interest (POIs) in the Enron corporate fraud case. We will acheive this goal by finding the correct machine learning algorithm to identify persons of interest.  The goal is to acheive a Precision and Recall greater than .3.    \n",
    "  \n",
    "  I started exploring the dataset and found some basic stats.  The dataset contains 146 data points with 21 features. The data points are people within the enron organization. The features were broken down by financial features and email features. Of the 146 people in the data set, 18 of them were already marked as Persons of Interest (POI's). \n",
    "  \n",
    "  Next, I started exploring the dataset to see what type of outliers I might find.  The first outliers I found was 'TOTAL' and 'The travel agency in the park'.  I removed both of them. I removed the 'TOTAL' line because that's a sum for spreadsheet, and will interfere with our investigation.  I also removed \"The Travel Agency in the Park\" which did not seem to ba an individual at Enron. I also found several outliers when it came to Bonuses and Salary, but kept those outliers because they could help identify Persons of Interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "#### Answer 2\n",
    "I created two new features: 'from_poi_ratio' and 'to_poi_ratio'.  I chose these two because I figured POI's would have a lot of contact with each other, and I thought it would be interesting to see that relationship.\n",
    "\n",
    "In order to find the most effective features for classification I decided to use “Decision Tree” to rank the features. First I put all the possible features into features_list including the new features I created.  I received and accuracy rating of 60%.  I then decided to run just the top ten of my previous decision tree.  It ranked the top 10 and I recevied an accuracy rating of 80%.  So, I decided to run with this feature list.\n",
    "\n",
    ">Accuracy: 0.8 <br>\n",
    "Decision tree algorithm time: 0.003 s <br>\n",
    "Feature Ranking: <br> \n",
    "1 feature salary (0.245695063538) <br>\n",
    "2 feature bonus (0.212292872081) <br>\n",
    "3 feature from_poi_ratio (0.149133225481) <br>\n",
    "4 feature to_poi_ratio (0.145084636726) <br>\n",
    "5 feature deferral_payments (0.0999902353286) <br>\n",
    "6 feature total_payments (0.0792993720256) <br>\n",
    "7 feature loan_advances (0.0685045948204) <br>\n",
    "8 feature restricted_stock_deferred (0.0) <br>\n",
    "9 feature deferred_income (0.0) <br>\n",
    "10 feature total_stock_value (0.0) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "#### Answer 3\n",
    "I tried Naive Bayes, Decision Tree and Adaboost with default parameters and without any tuning.  I tested these algorithms with the same training and test data and got different results.  These algorithms produced different accuracy ratings, precision and recall.  Based on those results, I decided that Decision Tree will be the best option to go with.  Then I took Decision Tree and started trying different parameters for tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "#### Answer 4\n",
    "Tuning the parameters of an algorithm means testing out different parameter values to find the combination that optimizes the result of the algorithm.  The purpose of this tuning is to find the best performance.  If we do not tune the parameters we might not get the best performance from the algorithm.\n",
    "\n",
    "My final algorithm was decision tree, I tried differnt parameters to tune the algorithm and found that 'entropy' gave me a better accuracy, precision and recall. \n",
    "     \n",
    "Decision Tree, I used the default parameters without any tuning.  (Close to the .3 goal, but not quite)\n",
    ">Accuracy:  0.81 <br>\n",
    "Precision:  0.29 <br>\n",
    "Recall:  0.4 <br>\n",
    "F1:  0.33 <br>\n",
    " <br>\n",
    "     \n",
    "I then tried Decison tree with 'entropy' as a criterion.  Entropy controls how a Decision Tree decides to split the data.  With this I have achieved a precision> 0.30 and a recall > 0.30.\n",
    "><strong>Decision Tree (Entropy)</strong> <br>\n",
    "Accuracy:  0.84 <br>\n",
    "Precsion:  0.38 <br>\n",
    "Recall:  0.60 <br>\n",
    "F1:  0.46 <br>\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "\n",
    "#### Answer 5\n",
    "Validation is the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model.  The classic mistake in validation is overfitting. \n",
    "\n",
    "I separated the data into training, testing, and validation sets. Test size was 30% of the data and Train size was 70% of the data.  This was done by using train_test_split in sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "The evaluation metrics I used were precision, recall, and F1.\n",
    "\n",
    "Precision <br>\n",
    ">Precision = (True Positive) / (True Positive+False positive) <br>\n",
    ">My precision is 38%, which means that 38% of the people were positively identified as POI's\n",
    "\n",
    "Recall <br>\n",
    ">Recall = (True Positives) / (True Positives+False Negatives) <br>\n",
    ">My recall is 60%, which means that this decision tree correctly identified 60% as being a POI<br>\n",
    ">I believe a higher recall is a better metric in this situation because it will tag the true POI's more efficiently and have a less likely chance of tagging innocent people as POI's\n",
    "\n",
    "\n",
    "F1 <br>\n",
    ">F1 = 2 * (Precision*Recall) / (Precision+Recall) <br>\n",
    ">F1 combines the precision and recall.  Mt F1 score is 46%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<ul>\n",
    "    <li>https://stackoverflow.com/questions/53899066/what-could-be-the-reason-for-typeerror-stratifiedshufflesplit-object-is-not</li>\n",
    "    <li>https://towardsdatascience.com/understanding-hyperparameters-optimization-in-deep-learning-models-concepts-and-tools-357002a3338a</li>\n",
    "    <li>https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9</li>\n",
    "    <li>https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-9863-7_233</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python 2]",
   "language": "python",
   "name": "conda-env-Python_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
